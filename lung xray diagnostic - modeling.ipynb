{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"63a6fd802c7d4be5a249c8de30184520":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a75d0ea37e7e4669b293d0b924579c11","IPY_MODEL_dc472f62086049b988da482d4436ca6e","IPY_MODEL_3c4b61e3815b4984bd8e0baa06064a45"],"layout":"IPY_MODEL_5a40cf7cd34c44e8a6f217244b6357aa"}},"a75d0ea37e7e4669b293d0b924579c11":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c3e714159f254af3b5a63ab301b2afc0","placeholder":"​","style":"IPY_MODEL_570e1f56f4d84660bf6d72053898d6fd","value":"Resolving data files: 100%"}},"dc472f62086049b988da482d4436ca6e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4682250a9c63456694e2366020238c02","max":2048,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7a593ae2f6e24ed482108dda5dfbb828","value":2048}},"3c4b61e3815b4984bd8e0baa06064a45":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5a04212a27f7444dae1dbbfb57312445","placeholder":"​","style":"IPY_MODEL_d607fd32258d4ca8a0febcd461744ba6","value":" 2048/2048 [00:00&lt;00:00, 36283.95it/s]"}},"5a40cf7cd34c44e8a6f217244b6357aa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c3e714159f254af3b5a63ab301b2afc0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"570e1f56f4d84660bf6d72053898d6fd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4682250a9c63456694e2366020238c02":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7a593ae2f6e24ed482108dda5dfbb828":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5a04212a27f7444dae1dbbfb57312445":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d607fd32258d4ca8a0febcd461744ba6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# Classification of Pneumonia from Lung Scans\n","\n","This project is an example of image classification - each input instance is one image, and the output is a categorical label.\n","\n","As example, we will perform classification on scans of patients' lungs into normal or pneumonia. The complete dataset is available at https://www.kaggle.com/datasets/tawsifurrahman/covid19-radiography-database however is very big. For demonstration purpose, we will only build a model for about 2000 images, half normal and half pneumonia. The data also has xrays of lungs diagnosed with COVID-19 and lung-opacity, however, we will not use them.\n","\n","This notebook can be used for any image classification task, just make sure that you have the same data organization:\n","- Images in the same class are stored in a same folder\n","- The folders' names are the labels\n","- The data is zipped into a single file"],"metadata":{"id":"VOOZq6Bc2CpO"}},{"cell_type":"markdown","source":["# Load data\n","\n","Like usual, we first connect our session to Google Drive. Make sure to upload the **lung images.zip** (available on D2L) to a folder there."],"metadata":{"id":"JZxqCVj7s11T"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jJhIaLI_sBIe","executionInfo":{"status":"ok","timestamp":1693949043111,"user_tz":240,"elapsed":18762,"user":{"displayName":"Linh Le","userId":"00514130171715816639"}},"outputId":"275a53c4-bf17-4b7a-eeb9-94bd0f63f863"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["Next, we need to unzip the data. We can use the `unzip` command, however, this is an operating system command, not Python, so we need to add `!` like below. Following `!unzip` is the path to the zipped data. After unzipping, verify that you have the new folder created."],"metadata":{"id":"o3AOkc3W93N4"}},{"cell_type":"code","source":["!unzip '/content/drive/MyDrive/IT7133/Week 4/lung images.zip'"],"metadata":{"id":"TtWfaYlusqAA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Process data\n","\n","Like any types of data, image data also needs processing. The cell below performs all the necessary processing for you, including\n","- standardize the images. Normal pixel values are from `0` to `255`. Standardization transforms them to be between `0` and `1` which is more preferrable by neural networks\n","- random crop, rotate, and zoom. These processes generate more training data and make our models more robust\n","\n","However, you do not have to worry too much about the codes in this cell. The images should be processed automatically and get ready for modeling. There will be a lot of outputs from this cell since we also install a few Python packages."],"metadata":{"id":"ZUfALuKM-xFh"}},{"cell_type":"code","source":["!pip install datasets evaluate transformers\n","\n","import PIL, datasets, evaluate\n","from os import listdir\n","from os.path import isfile, join\n","from torchvision.datasets import ImageFolder\n","from datasets import load_dataset\n","\n","dataset = load_dataset(\"imagefolder\", data_dir=\"lung images/\")\n","dataset = dataset['train'].train_test_split(test_size=0.3)\n","labels = dataset[\"train\"].features[\"label\"].names\n","label2id, id2label = dict(), dict()\n","for i, label in enumerate(labels):\n","    label2id[label] = str(i)\n","    id2label[str(i)] = label\n","\n","from transformers import AutoImageProcessor\n","checkpoint = \"google/vit-base-patch16-224-in21k\"\n","image_processor = AutoImageProcessor.from_pretrained(checkpoint)\n","\n","from tensorflow import keras\n","from keras import layers\n","import numpy as np\n","import tensorflow as tf\n","from PIL import Image\n","from transformers import DefaultDataCollator\n","import evaluate\n","import numpy as np\n","\n","size = (image_processor.size[\"height\"], image_processor.size[\"width\"])\n","train_data_augmentation = keras.Sequential(\n","    [\n","        layers.RandomCrop(size[0], size[1]),\n","        layers.Rescaling(scale=1.0 / 127.5, offset=-1),\n","        layers.RandomFlip(\"horizontal\"),\n","        layers.RandomRotation(factor=0.02),\n","        layers.RandomZoom(height_factor=0.2, width_factor=0.2),\n","    ],\n","    name=\"train_data_augmentation\",\n",")\n","val_data_augmentation = keras.Sequential(\n","    [\n","        layers.CenterCrop(size[0], size[1]),\n","        layers.Rescaling(scale=1.0 / 127.5, offset=-1),\n","    ],\n","    name=\"val_data_augmentation\",\n",")\n","\n","def convert_to_tf_tensor(image: Image):\n","    np_image = np.array(image)\n","    tf_image = tf.convert_to_tensor(np_image)\n","    return tf.expand_dims(tf_image, 0)\n","\n","def preprocess_train(example_batch):\n","    images = [\n","        train_data_augmentation(convert_to_tf_tensor(image.convert(\"RGB\"))) for image in example_batch[\"image\"]\n","    ]\n","    example_batch[\"pixel_values\"] = [tf.transpose(tf.squeeze(image)) for image in images]\n","    return example_batch\n","\n","def preprocess_val(example_batch):\n","    images = [\n","        val_data_augmentation(convert_to_tf_tensor(image.convert(\"RGB\"))) for image in example_batch[\"image\"]\n","    ]\n","    example_batch[\"pixel_values\"] = [tf.transpose(tf.squeeze(image)) for image in images]\n","    return example_batch\n","\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    predictions = np.argmax(predictions, axis=1)\n","    return accuracy.compute(predictions=predictions, references=labels)\n","\n","accuracy = evaluate.load(\"accuracy\")\n","dataset[\"train\"].set_transform(preprocess_train)\n","dataset[\"test\"].set_transform(preprocess_val)\n","data_collator = DefaultDataCollator(return_tensors=\"tf\")"],"metadata":{"id":"U4JGMSLXwGJK","colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["63a6fd802c7d4be5a249c8de30184520","a75d0ea37e7e4669b293d0b924579c11","dc472f62086049b988da482d4436ca6e","3c4b61e3815b4984bd8e0baa06064a45","5a40cf7cd34c44e8a6f217244b6357aa","c3e714159f254af3b5a63ab301b2afc0","570e1f56f4d84660bf6d72053898d6fd","4682250a9c63456694e2366020238c02","7a593ae2f6e24ed482108dda5dfbb828","5a04212a27f7444dae1dbbfb57312445","d607fd32258d4ca8a0febcd461744ba6"]},"executionInfo":{"status":"ok","timestamp":1693951702509,"user_tz":240,"elapsed":6256,"user":{"displayName":"Linh Le","userId":"00514130171715816639"}},"outputId":"1f03ca48-1ce7-44dc-b531-aec9e97390d2"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Resolving data files:   0%|          | 0/2048 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63a6fd802c7d4be5a249c8de30184520"}},"metadata":{}}]},{"cell_type":"markdown","source":["# Modeling\n","\n","In this phase, we build a neural network for the image classification task. We will use an auto model -- the library selects the architecture for us. So, we just need to set a few parameters:\n","- `num_epochs`: like in the previous module, this is the number of iteration\n","- `learning_rate`: how fast the model will update in each iteration"],"metadata":{"id":"hPg12qGY_wAN"}},{"cell_type":"code","source":["num_epochs = 3\n","learning_rate = 3e-5"],"metadata":{"id":"xzpZ_ofCAKyu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import create_optimizer, TFAutoModelForImageClassification\n","from keras.losses import SparseCategoricalCrossentropy\n","from transformers.keras_callbacks import KerasMetricCallback\n","\n","batch_size = 32\n","num_train_steps = len(dataset[\"train\"]) * num_epochs\n","weight_decay_rate = 0.01\n","\n","optimizer, lr_schedule = create_optimizer(\n","    init_lr=learning_rate,\n","    num_train_steps=num_train_steps,\n","    weight_decay_rate=weight_decay_rate,\n","    num_warmup_steps=0,\n",")\n","\n","model = TFAutoModelForImageClassification.from_pretrained(\n","    checkpoint,\n","    id2label=id2label,\n","    label2id=label2id,\n",")\n","\n","tf_train_dataset = dataset[\"train\"].to_tf_dataset(\n","    columns=\"pixel_values\", label_cols=\"label\", shuffle=True, batch_size=batch_size, collate_fn=data_collator\n",")\n","\n","tf_eval_dataset = dataset[\"test\"].to_tf_dataset(\n","    columns=\"pixel_values\", label_cols=\"label\", shuffle=True, batch_size=batch_size, collate_fn=data_collator\n",")\n","\n","loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","model.compile(optimizer=optimizer, loss=loss)\n","metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_eval_dataset)\n","callbacks = [metric_callback]\n","model.fit(tf_train_dataset, validation_data=tf_eval_dataset, epochs=num_epochs, callbacks=callbacks)"],"metadata":{"id":"EvlpfpvbwxBB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1693949984779,"user_tz":240,"elapsed":460235,"user":{"displayName":"Linh Le","userId":"00514130171715816639"}},"outputId":"f7981ab8-6582-4d06-96b6-2644702cb720"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some layers from the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing TFViTForImageClassification: ['vit/pooler/dense/bias:0', 'vit/pooler/dense/kernel:0']\n","- This IS expected if you are initializing TFViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some layers of TFViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","45/45 [==============================] - 169s 3s/step - loss: 0.4082 - val_loss: 0.2658 - accuracy: 0.9138\n","Epoch 2/3\n","45/45 [==============================] - 137s 3s/step - loss: 0.1877 - val_loss: 0.1697 - accuracy: 0.9415\n","Epoch 3/3\n","45/45 [==============================] - 147s 3s/step - loss: 0.1618 - val_loss: 0.1299 - accuracy: 0.9593\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7acbcff6f2e0>"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["# Save the Model\n","\n","Like in the previously, if we like the model, we will save it for deployment."],"metadata":{"id":"AvEAQ9sOATy_"}},{"cell_type":"code","source":["model.save_pretrained(\"/content/drive/MyDrive/IT7133/Week 4/lung_xray_model\")"],"metadata":{"id":"lGxYqZmgFTgz"},"execution_count":null,"outputs":[]}]}